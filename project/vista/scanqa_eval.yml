pipeline:
  name: optimus_prime

  task: scanqa
  eval_task: True
  restore_model: True

  qa_dataset:
    name: scanqa_task
    args:
      tokenizer: bert_tokenizer
      txt_seq_length: 50
      pc_seq_length: 135  # 80
      pc_type: 'gt'
      # pc_type: 'pred'  # 'gt'
      # split: 'val'  # 'test_wo_obj'
      # split: 'test_w_obj'  # 'test_wo_obj'
  
  lang_encoder:
    name: bert_lang_encoder
    args:
        num_hidden_layer: 4
     
  point_encoder:
    name: point_tokenize_encoder
    args:
      backbone: pointnet++
      hidden_size: 768
      path: null
      freeze_feature: True
      num_attention_heads: 12
      spatial_dim: 5
      num_layers: 4
      dim_loc: 6
      pairwise_rel_type: center

  unified_encoder:
    name: unified_encoder_v2
    args:
      hidden_size: 768
      num_attention_heads: 12
      num_layers: 4
      dim_loc: 6
      freeze: False

  ground_head:
    name: ground_head_v1
    args:
      input_size: 768
      hidden_size: 768
      sem_cls_size: 607
      dropout: 0.3

  qa_head:
    name: qa_head_v1
    args:
      num_answers: 8864

  pretrain_head:
    name: pretrain_head_v1
    args:
      hidden_size: 768
      vocab_size: 30522

  caption_head:
    name: caption_head_v1
    args:
      hidden_size: 768
      vocab_size: 4231

  supervise_base_head:
    name: supervise_base_head_v1
    args:
      hidden_size: 768

  coarse_ground_head:
    name: coarse_ground_head_v4
    args:
      hidden_size: 768

  fine_ground_head:
    name: fine_ground_head_v4
    args:
      hidden_size: 768
  
  inference_head:
    name: inference_head_v4
    args:
      hidden_size: 768

  qa_loss:
    name: qa_loss_v1
  
  logger:
    name: tensorboard_logger
    args:
      log_dir: ../runs/
  
  saver:
    name: model_saver
    args:
      # load_dir: project/pretrain_weights
      # load_name: pretrain.pth
      load_dir: ckpts
      load_name: eqa_235_5x5_ft7_2383.pth
      # load_name: stepwise_head_235_eqa_1att.pth
      save_dir: ckpts
      save_name: eval_abl.pth

  batch_size: 1
  learning_rate: 1.0e-6
  grad_norm: 5.0
  epochs: 50
  warmup_steps: 2000
  lang_lr_mul: 0.1
  point_lr_mul: 1.0
  unified_lr_mul: 1.0
  beta1: 0.9
  beta2: 0.98
  